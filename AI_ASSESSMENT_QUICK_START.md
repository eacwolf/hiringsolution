# Quick Start: AI Questions in Assessment

## Before You Start

âœ… **Backend running**: `cd backend && npm run dev`
âœ… **Frontend running**: `cd frontend && npm run dev`
âœ… **Environment variables**: Backend has `OPENAI_API_KEY` in `.env`

---

## End-to-End Test (5 minutes)

### Step 1: Create AI-Generated Questions âš™ï¸

1. Open http://localhost:5174/
2. Login with test account (or create one)
3. Click **Dashboard** â†’ **"Create New Exam"**
4. Fill form:
   - **Title**: "Backend Engineer Test"
   - **Domain**: Engineering
   - **Skill**: Java
   - **Difficulty**: Easy
   - Keep defaults for other fields
5. Click **"Generate Questions"** â³ (Wait 15-30 seconds)
6. Should see 3 questions generated

### Step 2: Verify Questions Were Created âœ…

After generation completes:
- Page navigates to `/questions`
- See **3 generated questions** listed
- Each shows:
  - Title (e.g., "Two Sum")
  - Description
  - Difficulty badge
  - View button

Click **View** on any question to see full details:
- Problem statement
- Constraints
- Test cases with explanations
- Reference solution
- Hints

**âœ“ Success**: All questions display with rich content

---

### Step 3: Take Assessment ğŸ§‘â€ğŸ’»

1. From Dashboard, click **"ğŸ“ Take Assessment"**
2. **Pre-Assessment Screen**:
   - Shows "3 Questions" stat
   - Enter name and email
   - Click **"Start Assessment"**
3. See **First Question** with:
   - Title: "Two Sum"
   - Full problem statement
   - Constraints (e.g., "2 <= nums.length <= 10^4")
   - Test cases (e.g., `nums = [2,7,11,15], target = 9` â†’ `[0,1]`)
   - Optional: Click hints or reference solution

### Step 4: Write and Test Code ğŸ’»

For the **Two Sum problem**, write simple code:

**Java**:
```java
public class Solution {
    public int[] twoSum(int[] nums, int target) {
        java.util.Map<Integer, Integer> map = new java.util.HashMap<>();
        for (int i = 0; i < nums.length; i++) {
            int complement = target - nums[i];
            if (map.containsKey(complement)) {
                return new int[]{map.get(complement), i};
            }
            map.put(nums[i], i);
        }
        return new int[]{};
    }
}
```

**Python**:
```python
def twoSum(nums, target):
    map_dict = {}
    for i, num in enumerate(nums):
        complement = target - num
        if complement in map_dict:
            return [map_dict[complement], i]
        map_dict[num] = i
    return []
```

### Step 5: Run Tests â–¶ï¸

1. Click **"Run Code"** button
2. Code sent to backend at `/api/code/execute`
3. Should see **Test Results**:
   ```
   âœ“ Test 1: nums = [2,7,11,15], target = 9
   âœ“ Test 2: nums = [3,2,4], target = 6
   âœ“ Test 3: nums = [3,3], target = 6
   
   2/2 Passed (100%)
   ```

### Step 6: Submit Solution ğŸ“¤

1. Click **"Submit Solution"** button
2. Solution evaluated:
   - Test results scored
   - Code quality analyzed (formatting, comments, complexity)
   - Final score calculated
3. Popup shows: `"Solution submitted! Score: 95%"`
4. Automatically moves to **Question 2**

### Step 7: Complete Remaining Questions

Repeat Steps 4-6 for Questions 2 and 3

### Step 8: View Rankings ğŸ†

After submitting all 3 questions:
1. Automatically redirected to `/submissions-ranking`
2. See **Leaderboard** with:
   - Your submission ranked
   - Score breakdown
   - Comparison with other candidates
3. Click **"View Details"** to see:
   - Your code vs reference solution
   - Test results breakdown
   - Code quality metrics

---

## What Gets Displayed

### Question Details (Each Problem Shows)

âœ… Problem title and description
âœ… Full problem statement (what to solve)
âœ… Constraints (algorithm limits)
âœ… 2-3 test cases with:
   - Input example
   - Expected output
   - Explanation why
âœ… Hints (collapsible)
âœ… Working reference solution (collapsible)
âœ… Algorithm explanation including complexity

### Assessment Interface

âœ… Side-by-side editor (left: problem, right: code)
âœ… Language selector (Java/Python toggle)
âœ… Progress bar showing 1 of 3 questions
âœ… Color-coded test results (âœ“ pass, âœ— fail)
âœ… Navigation between questions
âœ… Score in real-time

### Rankings Display

âœ… Leaderboard table with:
   - Rank with medals (ğŸ¥‡ ğŸ¥ˆ ğŸ¥‰)
   - Candidate name and email
   - Questions solved
   - Tests passed
   - Final score
âœ… Statistics cards (total submissions, avg score, etc.)
âœ… Filters by difficulty and language
âœ… Detailed submission modal

---

## Expected Questions (AI-Generated)

### Easy Level
- **Two Sum** - Find two numbers summing to target
- **Palindrome Check** - Or similar string problem
- **Array Operations** - Basic array manipulation

### Medium Level (if difficulty: Medium selected)
- **Longest Substring Without Repeating Characters**
- **LRU Cache Implementation**
- **Binary Tree Traversal**

### Hard Level (if difficulty: Hard selected)
- **Median of Two Sorted Arrays**
- **Word Ladder**
- **Longest Valid Parentheses**

---

## Troubleshooting

### âŒ "Generating questions..." takes too long

**Cause**: OpenAI API slow or backend not connected
**Solution**:
- Wait 30+ seconds
- Check backend console for errors
- Verify `OPENAI_API_KEY` is set correctly
- Check internet connection

### âŒ "No questions available" on assessment page

**Cause**: Questions weren't generated yet
**Solution**:
1. Create exam first (see Step 1)
2. Wait for generation to complete
3. Refresh page and try assessment again

### âŒ Test results show all failures

**Cause**: Mock code executor doesn't recognize your code
**Solution**:
1. Current version uses regex-based validation
2. Use reference solution as template
3. Ensure code contains expected patterns (for, if, def, etc.)

### âŒ Code runs fine locally but fails in assessment

**Cause**: Test case input format differs
**Solution**:
1. Check test case input carefully
2. Different from your local test
3. Click "View Details" to see exact input/output
4. Copy test case directly from problem

---

## What to Look For âœ…

- [x] Questions have realistic problem titles (not generic)
- [x] Problem statements are detailed and clear
- [x] Constraints are realistic and specific
- [x] Test cases have actual numbers/strings (not placeholders)
- [x] Reference solutions are runnable code (not pseudo-code)
- [x] Code quality metrics are calculated
- [x] Leaderboard shows candidates ranked by performance
- [x] Scoring is fair and consistent

---

## Performance Metrics to Check

| Metric | Expected |
|--------|----------|
| Question generation | 15-30 seconds |
| Assessment page load | <1 second |
| Test result display | <2 seconds |
| Ranking page load | <1 second |
| Submit and next question | <1 second |

---

## Success Criteria: Assessment Complete âœ“

âœ… Generated 3 AI questions with full problem context
âœ… Viewed questions with test cases and hints
âœ… Wrote code in Java/Python
âœ… Ran code against test cases
âœ… Submitted solution with score
âœ… Saw all questions in assessment
âœ… Viewed rankings and leaderboard
âœ… All scores calculated correctly

**If all above work â†’ System is ready!** ğŸ‰
